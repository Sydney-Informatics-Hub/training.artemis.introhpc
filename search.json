[
  {
    "objectID": "CHEATSHEET.html",
    "href": "CHEATSHEET.html",
    "title": "Quarto template cheatsheet",
    "section": "",
    "text": "Quarto template cheatsheet\nPlease contribute your tips, tricks for use, customisation of this template :)"
  },
  {
    "objectID": "notebooks/02_connect.html",
    "href": "notebooks/02_connect.html",
    "title": "Introduction to High Performance Computing",
    "section": "",
    "text": "Linux background\nArtemis runs on a linux operating system (currently CentOS v6.10). Becoming familar with linux commands can be quite challenging for those users who are used to graphical systems such as Windows. A Basic knowledge of linux is a preferrable for this course. A recommended course to learn the basics is by Software Carpentry.\nUsing Linux involves interacting with the system through commands entered in a command-line interface (CLI). The pattern of commands tend to follow:\ncommand [options] [arguments]\nFor instance, This command on your local terminal ls -ltr ~ lists all files in the home directory.\n(ls) is the command. Options include: long format (-l), sorted by modification time (-t) in reverse order with the most recently modified files appearing at the bottom (-r). The argument is (~) which is shorthand for the home directory.\nnano is the text editor we will be using to make and adjust files and is preinstalled on Artemis. Using nano will be demonstrated.\n\n\nThe Scheduler.\nOn Artemis, PBS Pro is the name of the scheduler that manages computer resources and jobs (what people want to run). The scheduler manages available capacity and the competing jobs wanting to access its resources via a queue. One a job is submitted to the scheduler (via qsub command ), a job is placed in the queue until spare capacity is found to run it. Generally the larger the resources you are requesting, the longer your job will be placed in a queue.\nPBS Scripts are how we communicate our requirements to the scheduler. PBS Declarations are the syntax of doing so and is a combination of a directive, a flag and options. All lines starting with #PBS are PBS declarations and the scheduler will try to interpret its meaning. An example of a PBS declaration is:\n#PBS -l select=1:ncpus=8:mem=16gb\nThis specifies 1 node (select=1), with 8 CPUs (ncpus=8) and 16 GB of RAM (mem=16GB). A series of PBS declarations are housed within a PBS script.\nRequesting resources is a combination of:\nselect: the number of compute nodes (most often 1 unless MPI is being used)\nncpus: the number of CPU cores\nmem: the amount of RAM\nngpus: the number of GPU cores\nwalltime: the length of time all these resources will be made available to the requesting job\nCommon PBS Flags are:\n\n\n\n\n\n\n\n\nOption\nDescription\nNotes\n\n\n\n\n-P\nProject short name\nRequired directive on Artemis\n\n\n-N\nJob name\nName it whatever you like; no spaces\n\n\n-l\nResource request\na comination of resources as mentioned\n\n\n-q\nJob queue\nDefaults to defaultQ. dtq for data transfer only\n\n\n-M\nEmail address\nNotifications can be sent by email on certain events\n\n\n-m\nEmail options: abe\nReceive notification on (a)bort, (b)egin, or (e)nd of jobs\n\n\n-I\nInteractive mode\nOpens a shell terminal with access to the requested resources\n\n\n\nOther HPC systems might use other Schedulers requiring a different syntax (SLURM), but the concepts are the same.\nA mention on Home versus Compute Nodes\nOnce you logon on to Artemis you are placed on the logon nodes. Avoid running scripts there as you wont be able to access the potential resources available. PBS script submitted to the scheduler ensure your job is transfered from the logon nodes to its compute nodes that access larger resources. This will be demonstrated."
  },
  {
    "objectID": "notebooks/03_step4.html",
    "href": "notebooks/03_step4.html",
    "title": "Introduction to High Performance Computing",
    "section": "",
    "text": "Research Computing Wiki\nLearning Linux via Software Carpentry Course\nHPC Options other than Artemis\nUniversity of Sydney Research Cloud\nNCI\nSydney Informatics Hub HPC Scheme"
  },
  {
    "objectID": "notebooks/03_step4.html#more-resources",
    "href": "notebooks/03_step4.html#more-resources",
    "title": "Introduction to High Performance Computing",
    "section": "",
    "text": "Research Computing Wiki\nLearning Linux via Software Carpentry Course\nHPC Options other than Artemis\nUniversity of Sydney Research Cloud\nNCI\nSydney Informatics Hub HPC Scheme"
  },
  {
    "objectID": "notebooks/03_step3.html",
    "href": "notebooks/03_step3.html",
    "title": "Introduction to High Performance Computing",
    "section": "",
    "text": "Storing data on Artemis is not recommended due to the automatic cleaning of unused data, and the fact that data is not backed up. Its hardware is geared towards processing (compute) rather than long term storage.\nFor data storage, we have a dedicated set of machines referred to as the Research Data Store (RDS). The RDS is a central location to securely store research data for a long duration.\nAccess to RDS is granted and managed through a “Research Data Managment Plan” project completed in the Research Dashboard DASHR. More information on RDS can be found here.\nGenerally there are three ways you can transfer files to RDS.\n\nUse commands (scp or rsync) or a graphical based app (FileZilla) to move files to its remote location.\nTransfer files from Artmeis to RDS, as the RDS appears as an accessible drive on it.\nMount your RDS to your local computer.\n\nLets practice data transfer by moving the download.pbs file. “Download ZIP” under the green code button can be used to download it if you are unfamiliar with git."
  },
  {
    "objectID": "notebooks/03_step3.html#research-data-storage",
    "href": "notebooks/03_step3.html#research-data-storage",
    "title": "Introduction to High Performance Computing",
    "section": "",
    "text": "Storing data on Artemis is not recommended due to the automatic cleaning of unused data, and the fact that data is not backed up. Its hardware is geared towards processing (compute) rather than long term storage.\nFor data storage, we have a dedicated set of machines referred to as the Research Data Store (RDS). The RDS is a central location to securely store research data for a long duration.\nAccess to RDS is granted and managed through a “Research Data Managment Plan” project completed in the Research Dashboard DASHR. More information on RDS can be found here.\nGenerally there are three ways you can transfer files to RDS.\n\nUse commands (scp or rsync) or a graphical based app (FileZilla) to move files to its remote location.\nTransfer files from Artmeis to RDS, as the RDS appears as an accessible drive on it.\nMount your RDS to your local computer.\n\nLets practice data transfer by moving the download.pbs file. “Download ZIP” under the green code button can be used to download it if you are unfamiliar with git."
  },
  {
    "objectID": "notebooks/03_step3.html#graphical-transfer",
    "href": "notebooks/03_step3.html#graphical-transfer",
    "title": "Introduction to High Performance Computing",
    "section": "Graphical Transfer",
    "text": "Graphical Transfer\n\nTransfer to Artemis\nDownload FileZilla Client which is a cross platform app to transfer data to remote sites.\n\nIn the Host field enter sftp://hpc.sydney.edu.au. This is Artemis and files can be transfered from your local computer to here via a secure file transfer protocol.\nFor your Username field enter the same username that you used previously for artemis associated with training. i.e. ict_hpctrainN where N is your number between 1 and 20. In the Password field, enter the training password.\nThe Port field should by default be 22 but sometimes its better to specify it.\n\nOnce connected, you should see the home directory of Artemis on the right pane. Let navigate to the same directory we used earlier. To change to a different directory, in the Remote Site field type /project/Training/. The right pane should then reflect its contents. Navigate to the same subdirectory you used earlier that sits under Training, i.e. /project/Training/&lt;yourDirectoryName&gt;.\nOnce the appropriate locations are found, to remotely copy a file drag and drop contents from the left pane to the right.\n\n\nTransferring a file to RDS\nUsing Filezilla, transferring a file to RDS follows a similar process. You first need to connect to the RDS by:\n\nIn the Host field enter either research-data-ext.sydney.edu.au or research-data-int.sydney.edu.au. Choose the external location (first choice) if you are not on campus (and have your VPN up and running), otherwise the internal (second) choice if you are at campus.\nCredentials and Port number as before.\n\nOnce connected, in the Remote Site field type /rds/PRJ-Training and enter. Contents on the right pane should now be visible. Once you begin to use your own credentials for your own projects, use /rds/PRJ-NameOfProject instead, where the NameOfProject is the short name of your project as dashr knows it to be."
  },
  {
    "objectID": "notebooks/03_step3.html#using-commands",
    "href": "notebooks/03_step3.html#using-commands",
    "title": "Introduction to High Performance Computing",
    "section": "Using commands",
    "text": "Using commands\nEither scp or rsync commands in your Terminal (Mac and linux: Terminal, Windows: Windows Terminal or Powershell equivalent) can be used as an alternative to graphical applications. Secure copy (scp) is demonstrated below. You will be asked to provide a password (your key strokes will not be shown) and once verified, a status bar will then be displayed showing your copy progress.\nOn your Local Terminal type:\nscp dogScripts.tar.gz  ict_hpctrain&lt;N&gt;@hpc.sydney.edu.au:/project/Training/&lt;yourDirectoryName&gt;\nWhere  where N is your number between 1 and 20."
  },
  {
    "objectID": "notebooks/03_step3.html#moving-data-while-on-artemis",
    "href": "notebooks/03_step3.html#moving-data-while-on-artemis",
    "title": "Introduction to High Performance Computing",
    "section": "Moving data while on Artemis",
    "text": "Moving data while on Artemis\n\nUsing the data transfer queue\nCopying small files the above ways is great - except when the files are considerable large. Interuptions to your interconnect connection can corrupt a large transfer. If you are on Artemis, we can instead ‘wrap’ file transfers in a PBS script to ensure your transfer are not hampered by interuptions.\nLets practice. Using commands on the Artemis Terminal extract the compressed file by typing tar -zxvf dogScripts.tar.gz. Once files are extracted submit the file transfer to the scheudler by typing qsub download.pbs. Notice the in the download.pbs file queue specified is the datatransfer queue.\n\n\nTransfer between RDS and Artemis\nAs mentioned, the RDS appears as a mount on Artemis. Hence the usual linux commands like cp or rsync are the standard way to transfer files between Artemis and RDS. The following command on the Artemis Terminal demonstrates file transfer:\ncp -v /rds/PRJ-Training/Dog_disease/Ref/* /project/Training/&lt;yourname&gt;\nThere is an even easier way! Our HPC’s management team have created a handy utility for us called dt-script. This script (full path /usr/local/bin/dt-script for the interested) is a ‘wrapper’ for running rsync in a PBS script. It enables you to perform data transfers as a “one-liner” without the need for a script and the transfer is actually submitted to the data transfer nodes of the cluster (and not running in the foreground of your current terminal).\nThe syntax for using dt-script is dt-script -P  -f  -t \nwhere -P is the dashr project (short name), from (-f) directory and to (-t) a directory.\nThe following steps may work best on the Artemis Production Instance\nmodule unload training\nOn the Artemis Terminal:\ndt-script -P Training -f /rds/PRJ-Training/Dog_disease/Ref/ -t /project/Training/&lt;yourDirectoryName&gt;/\nThe reverse is the easiest way possible to back up your project on Artemis. Assuming you are situated within the directory containing the data to backup (so that you can make use of pwd shortcut). On the Artemis Terminal:\ndt-script -P Training -N backup&lt;yourName&gt; -f `pwd` -t /rds/PRJ-Training"
  },
  {
    "objectID": "notebooks/03_step3.html#mounting-your-rds-on-your-drive",
    "href": "notebooks/03_step3.html#mounting-your-rds-on-your-drive",
    "title": "Introduction to High Performance Computing",
    "section": "Mounting your RDS on your drive",
    "text": "Mounting your RDS on your drive\nKeep in mind you must have your VPN on or be at campus for connection to establish.\n\nThese steps for Windows 10 are:\n\nClick on This PC from the Desktop.\nOn the Computer tab, click on Map network drive in the Network section. Choose a drive letter and enter you the path: \\\\shared.sydney.edu.au\\research-data. When asked, please enter shared&lt;unikey&gt; for the user name.\nClick Finish.\n\n\n\nThese steps for Mac are:\nTo mount on Linux or Mac operating systems, you can use the smb network communication protocol (also known as CIFS) by mounting the path.\n\nFinder &gt; Go &gt; Connect to Server.\nIn the Server Address use: smb://shared.sydney.edu.au/research-data\nEnter your credentials as a Registered User.\n\n\n\nThese steps for Linux are:\nThe command line is the easiest way to mount the RDS is using a tool called sshfs which connects via the ssh protocol and performs file transfers using sftp.\n\nFirstly install sshfs sudo apt install sshfs.\nCreate a directory where you want the mounted data to reside mkdir /home/ubuntu/myRDS and mount away sshfs &lt;unikey&gt;@research-data-int.sydney.edu.au:/rds/PRJ-&lt;yourproject&gt; /home/ubuntu/myRDS."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to High Performance Computing",
    "section": "",
    "text": "Introduces Sydney’s High Performance Computer (HPC), ‘Artemis’. We cover connecting and navigating Artemis, available software, and how to submit and monitor jobs using the PBS Pro scheduler. Students and staff who would like to learn how to run compute jobs on Artemis HPC. Participants must have a valid Sydney University Unikey."
  },
  {
    "objectID": "index.html#watch-the-recording",
    "href": "index.html#watch-the-recording",
    "title": "Introduction to High Performance Computing",
    "section": "Watch the Recording",
    "text": "Watch the Recording\nThis was delivered as an interactive workshop at Sydney. If you find yourself here in the future feel free to watch the recording:\nComing Soon!"
  },
  {
    "objectID": "index.html#contributors",
    "href": "index.html#contributors",
    "title": "Introduction to High Performance Computing",
    "section": "Contributors",
    "text": "Contributors\n\nKris SIH"
  },
  {
    "objectID": "index.html#course-pre-requisites-and-setup-requirements",
    "href": "index.html#course-pre-requisites-and-setup-requirements",
    "title": "Introduction to High Performance Computing",
    "section": "Course pre-requisites and setup requirements",
    "text": "Course pre-requisites and setup requirements\nNo previous programming experience is required, this training course will introduce you to fundamentals of the various tools (unix/linux, HPC) as required. Training will be delivered online, so you will need access to a modern computer with a stable internet connection. Participants will require the Sydney VPN and a terminal client (as per the Setup Instructions provided)."
  },
  {
    "objectID": "index.html#venue",
    "href": "index.html#venue",
    "title": "Introduction to High Performance Computing",
    "section": "Venue",
    "text": "Venue\nOnline via Zoom, a link will be shared with registered participants.\n\nZoom etiquette and how we interact\nSessions will be recorded and added to this page. Please interrupt whenever you want! Ideally, have your camera on and interact as much as possible. There will be someone monitoring the chat-window for any questions you would like to post there. Extending for a three-hour duration, our Zoom session incorporates regular breaks and a blend of demonstrative and hands-on material."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Introduction to High Performance Computing",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nAs a University of Sydney course, we also want to make sure peopele are aware of our code of conduct. Feel free to move this to an about page as needed.\nExample standard Code of Conduct statement:\nWe expect all attendees of our training to follow our code of conduct, including bullying, harassment and discrimination prevention policies.\nIn order to foster a positive and professional learning environment we encourage the following kinds of behaviours at all our events and on our platforms:\n\nUse welcoming and inclusive language\nBe respectful of different viewpoints and experiences\nGracefully accept constructive criticism\nFocus on what is best for the community\nShow courtesy and respect towards other community members\n\nOur full CoC, with incident reporting guidelines, is available Here."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Introduction to High Performance Computing",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nIn-depth\n\n\n\n\nArtemis HPC"
  },
  {
    "objectID": "index.html#setup-instructions",
    "href": "index.html#setup-instructions",
    "title": "Introduction to High Performance Computing",
    "section": "Setup Instructions",
    "text": "Setup Instructions\nPlease attempt to complete the Setup Instructions before the course. We will run through this in the course too."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup",
    "section": "",
    "text": "To connect to Artemis HPC, and follow this lesson, you will need a ‘terminal emulator with SSH (secure shell). SSH provides encrypted communication between the your computer and a remote server, in this case Artemis. The SSH client will be invoked by the command ‘ssh’.\nIf you are a windows users please have one of either apps below installed:\n\nPutty (recommended)\nMobaXterm\n\nIf you are a mac user we will be using the Terminal which comes pre-installed on your device.\nFor all users, if you are not on campus install and/or have your Cisco Anyconnect VPN up and running."
  },
  {
    "objectID": "setup.html#essential-software",
    "href": "setup.html#essential-software",
    "title": "Setup",
    "section": "",
    "text": "To connect to Artemis HPC, and follow this lesson, you will need a ‘terminal emulator with SSH (secure shell). SSH provides encrypted communication between the your computer and a remote server, in this case Artemis. The SSH client will be invoked by the command ‘ssh’.\nIf you are a windows users please have one of either apps below installed:\n\nPutty (recommended)\nMobaXterm\n\nIf you are a mac user we will be using the Terminal which comes pre-installed on your device.\nFor all users, if you are not on campus install and/or have your Cisco Anyconnect VPN up and running."
  },
  {
    "objectID": "setup.html#credentials-to-access-artemis",
    "href": "setup.html#credentials-to-access-artemis",
    "title": "Setup",
    "section": "Credentials to Access Artemis",
    "text": "Credentials to Access Artemis\nYou would normally use your unikey and unikey password to access Artemis. For today however, we will be using training logon credentials which are:\nUsername: ict_hpctrain, with N from 1-20 (replace  with your assigned number)\nPassword: will be written on the whiteboard!\n\nMac Logon to Artemis\nUse the Terminal App. Just look for it in your Applications folder, or hit Command-Space and type ‘terminal’. Then in the terminal enter:\nssh  &lt;unikey&gt;@hpc.sydney.edu.au\n\n\nWindows Logon to Artemis\n\nOpen Putty.\nFill in the connection details:\nHost Name: hpc.sydney.edu.au\nPort: 22\nConnection type: SSH\nName this session “Artemis” and click ‘Save’"
  },
  {
    "objectID": "setup.html#working-on-artemis-with-graphical-applications",
    "href": "setup.html#working-on-artemis-with-graphical-applications",
    "title": "Setup",
    "section": "Working on Artemis with Graphical Applications",
    "text": "Working on Artemis with Graphical Applications\nX11 FORWARDING\nX11 forwarding allows you to run graphical applications on a remote server and have their graphical user interfaces (GUIs) displayed on your local machine. This is particularly useful when you are working on a remote server or computer and need to run graphical applications, such as graphical text editors, web browsers, or other GUI-based tools.\nTo use X11 forwarding you need:\n\nMac Users: Xquartz up and running, then log on to Artemis by providing the extra X flag. That is: ssh -X &lt;unikey&gt;@hpc.sydney.edu.au\nWindows Users: Xming should be up and running. In PuTTY for Windows, you can enable X11 forwarding in new or saved SSH sessions by selecting Enable X11 forwarding in the PuTTY Configuration window (Connection &gt; SSH &gt; X11).\n\nAlternatively for windows users, MobaXterm offers a rich experience as a full-featured X-server and terminal emulator for ssh connections, the free version is more than adequate.\nNOMACHINE\nAn alternative to X11 forwarding is to use our graphical login nodes via the NoMachine App. These special are graphics-enabled login servers which host ‘NoMachine’, a kind of remote desktop service.\nOur experience is that no-machine is preferrable when using visualisation software that rely on computation done on the HPC.\nTo use the graphical login nodes:\nDownload and install the NoMachine Enterprise Client for your operating system. Please don’t download the client from NoMachine directly, as their current version may not work with Artemis.\nIf you’re using MacOS, you need to install XQuartz.\nDownload the glogin.nxs session file to your computer.\nTo connect:\nDouble-click or run the glogin.nxs shortcut you downloaded (it will open NoMachine Client). A NoMachine window should start, asking for a username and password. Replace “YOUR_UNIKEY” with your UniKey, then enter your UniKey password in the password field, then click ‘OK’. A short while later, an Artemis terminal window should open."
  },
  {
    "objectID": "notebooks/03_step1.html",
    "href": "notebooks/03_step1.html",
    "title": "Introduction to High Performance Computing",
    "section": "",
    "text": "Assuming you are logged on to Artemis with the training credentials lets run compute jobs on Artemis HPC. Please use your own unikey credentials after todays training.\nAs mentioned, pbs scripts communicate your requirements to the scheduler. These scripts tend to follow the basic pattern:\n\na, Write pbs declarations to communicate with the scheduler.\nb, Load modules required for your scripts (including environments)\nc, Run your scripts and manage directory paths and/or shell variables\n\nThe below series of commands will create a folder in a specifc location with the /project folder, extract data and rename a file which we will use to run our scripts. As we have logged in with the training credentials, we should have access to the /project/Training folder and the compressed file housed within it.\nOn the Artemis Terminal type the following commands:\ncd /project/Training\nmkdir &lt;yourname&gt;\ncd &lt;yourname&gt;\ntar -xzvf /project/Training/DATA/intro_hpc.tar.gz\ncd datahpc\ncp index.pbs basic.pbs\nCreating and running pbs files are how you communicate the the HPC Scheduler which managed your script and the resources of the HPC. Refer to the HPC Context section for more info on what the scheduler does and implications in its use.\nOpen up nano and adjust the basic.pbs with nano basic.pbs. We will alter its contents to run a simple hello world script. Specify the Training project and defaultQ queue in the PBS declarations. Delete the existing content below the PBS declarations of basic and add the below content to create a hello world to test Artemis\nnano lines after PBS Declarations:\ncd /project/Training/datahpc/&lt;yourname&gt;\nmkdir New_job\ncp basic.pbs New_job/copy.pbs\nperl hello.pl \"YourName\"\nSubmit the basic.pbs script to the scheduler with qsub basic.pbs.\nHow do we know if our script ran successfully? Investigate the output and error files that were automatically generated whose name refers to the #PBS -N flag and includes the job number printed when qsub was engaged."
  },
  {
    "objectID": "notebooks/03_step2.html",
    "href": "notebooks/03_step2.html",
    "title": "Introduction to High Performance Computing",
    "section": "",
    "text": "Now that we know how to submit jobs to the scheduler lets reinforce this by running scripts that run different programming languages.\nInvestigate the computepi_fire.py python file. Given a number of trials to run, this code estimates the number pi by randomly assigning points within a square and comparing the number of points that fall within a circle of radius one relative to the total number of points.\nnano estimate_pi.pbs\nThings to note:\n\nThe cd $PBS_O_WORKDIR command changes the directory to the location where the pbs script was submitted. This is an example of a shell variable that has been set by the pbs environment.\nThe Multiprocessing package is used to create a pool of resources (with its owner python interpreter). This package is a solution to the notorious global interpreter lock GIL that prevents multiple threads from executing python at once.\nThe pbs script that runs computepi_fire.py requires a list of numbers reflecting a simulation number. The larger, the more accurate the approximation of pi. Now is a good chance to look at queue management and practice qstat -x &lt;jobnumber&gt;, qstat -u &lt;username&gt; and qdel &lt;jobnumber&gt; commands by increasing the simulation numbers.\n\n\n\n\nIf you are familiar with Matlab on your local machine, using Matlab is a little different here on Artemis. High performance computers generally work best when engaging software via Linux commands. Applications that rely on graphical interfaces generally are slow and buggy (Nomachine is recommended if there is no alternative).\nqsub airline.pbs\nThings to note:\n\nThe extra flags -nodisplay -nosplash suppress the matlab graphical interaction that become problematic on Artemis.\nThe command line way to run Matlab with the -r (run) flag. Notice the exit that closes the automatic graphical communication that Matlab automatically engages.\n\n\n\n\nRunning R code on Artemis requires installing and referencing libraries needed for your R script. The recommended way to do this is to use the renv package to replicate libraries and specific versions created locally via a “lockfile”. The lockfile is created by renv. Alternatives installing packages seperately (most likely using a script or interactive queue) and updating the libPaths within R scripts is the simpler option if renv isnt used.\nThree example scripts to demostrate using R (with an established renv lock file) are:\n\ninstall_env.pbs: Houses the initial setup of installing R libraries to a path you have read / write permissions to. Here /home is used, but in practice it should reside somewhere in your /project directory. The installation has already been run and libraries saved in the “/project/Training/DATA/Libs” folder\ncreate_env.R: Demonstrates how to load an existing lock file and install packages. This is triggered by install_env.pbs\nrun_network.pbs : Main pbs script that references a path where libraries exist\nnetwork.R : The main R script that does data manipulation and visualisation via common R libraries including dplyr, plotly and network libraries. Generates a network plot pdf based on flight information\n\nqsub run_network.pbs\nThings to note: 1. If you have installed packages in locations that R is not expecting, it needs to know where to look for installed packages. Updating this via libPaths does this and tells R where to look for libraries.\n.libPaths( c( .libPaths(), \"/project/Training/DATA/Libs\") )\n\nWith the pbs script, updating paths that R expect libraries to be in can be done via export command. Example export R_LIBS_USER=$HOME/R/library"
  },
  {
    "objectID": "notebooks/03_step2.html#practice-makes-perfect---python-r-and-matlab",
    "href": "notebooks/03_step2.html#practice-makes-perfect---python-r-and-matlab",
    "title": "Introduction to High Performance Computing",
    "section": "",
    "text": "Now that we know how to submit jobs to the scheduler lets reinforce this by running scripts that run different programming languages.\nInvestigate the computepi_fire.py python file. Given a number of trials to run, this code estimates the number pi by randomly assigning points within a square and comparing the number of points that fall within a circle of radius one relative to the total number of points.\nnano estimate_pi.pbs\nThings to note:\n\nThe cd $PBS_O_WORKDIR command changes the directory to the location where the pbs script was submitted. This is an example of a shell variable that has been set by the pbs environment.\nThe Multiprocessing package is used to create a pool of resources (with its owner python interpreter). This package is a solution to the notorious global interpreter lock GIL that prevents multiple threads from executing python at once.\nThe pbs script that runs computepi_fire.py requires a list of numbers reflecting a simulation number. The larger, the more accurate the approximation of pi. Now is a good chance to look at queue management and practice qstat -x &lt;jobnumber&gt;, qstat -u &lt;username&gt; and qdel &lt;jobnumber&gt; commands by increasing the simulation numbers.\n\n\n\n\nIf you are familiar with Matlab on your local machine, using Matlab is a little different here on Artemis. High performance computers generally work best when engaging software via Linux commands. Applications that rely on graphical interfaces generally are slow and buggy (Nomachine is recommended if there is no alternative).\nqsub airline.pbs\nThings to note:\n\nThe extra flags -nodisplay -nosplash suppress the matlab graphical interaction that become problematic on Artemis.\nThe command line way to run Matlab with the -r (run) flag. Notice the exit that closes the automatic graphical communication that Matlab automatically engages.\n\n\n\n\nRunning R code on Artemis requires installing and referencing libraries needed for your R script. The recommended way to do this is to use the renv package to replicate libraries and specific versions created locally via a “lockfile”. The lockfile is created by renv. Alternatives installing packages seperately (most likely using a script or interactive queue) and updating the libPaths within R scripts is the simpler option if renv isnt used.\nThree example scripts to demostrate using R (with an established renv lock file) are:\n\ninstall_env.pbs: Houses the initial setup of installing R libraries to a path you have read / write permissions to. Here /home is used, but in practice it should reside somewhere in your /project directory. The installation has already been run and libraries saved in the “/project/Training/DATA/Libs” folder\ncreate_env.R: Demonstrates how to load an existing lock file and install packages. This is triggered by install_env.pbs\nrun_network.pbs : Main pbs script that references a path where libraries exist\nnetwork.R : The main R script that does data manipulation and visualisation via common R libraries including dplyr, plotly and network libraries. Generates a network plot pdf based on flight information\n\nqsub run_network.pbs\nThings to note: 1. If you have installed packages in locations that R is not expecting, it needs to know where to look for installed packages. Updating this via libPaths does this and tells R where to look for libraries.\n.libPaths( c( .libPaths(), \"/project/Training/DATA/Libs\") )\n\nWith the pbs script, updating paths that R expect libraries to be in can be done via export command. Example export R_LIBS_USER=$HOME/R/library"
  },
  {
    "objectID": "notebooks/01_context.html",
    "href": "notebooks/01_context.html",
    "title": "Introduction to High Performance Computing",
    "section": "",
    "text": "Artemis is ideal for calculations that require:\n\nA long time to complete (long walltime)\nHigh RAM usage\nBig data input or outputs\nAre using specific software that is designed to be run on HPC Systems (multiple cores or nodes run in parallel)\n\nArtemis is available free of charge to all University of Sydney researchers. You do need a unikey, and a valid Research Dashboard Research Data Management Plan (DASR) with Artemis access enabled.\nArtemis is also a great incentive to funding bodies to view your projects favourably – as they know you have the resources required to get the work done."
  },
  {
    "objectID": "notebooks/01_context.html#why-use-artemis",
    "href": "notebooks/01_context.html#why-use-artemis",
    "title": "Introduction to High Performance Computing",
    "section": "",
    "text": "Artemis is ideal for calculations that require:\n\nA long time to complete (long walltime)\nHigh RAM usage\nBig data input or outputs\nAre using specific software that is designed to be run on HPC Systems (multiple cores or nodes run in parallel)\n\nArtemis is available free of charge to all University of Sydney researchers. You do need a unikey, and a valid Research Dashboard Research Data Management Plan (DASR) with Artemis access enabled.\nArtemis is also a great incentive to funding bodies to view your projects favourably – as they know you have the resources required to get the work done."
  },
  {
    "objectID": "notebooks/01_context.html#what-is-artemis",
    "href": "notebooks/01_context.html#what-is-artemis",
    "title": "Introduction to High Performance Computing",
    "section": "What is Artemis",
    "text": "What is Artemis\nArtemis is the University of Sydney’s High Performance Computer. Technically, Artemis is a computing cluster, which is a whole lot of individual computers networked together. At present, Artemis consists of:\n7,636 cores (CPUs)\n45 TB of RAM\n108 NVIDIA V100 GPUs\n378 TB of storage\n56 Gbps FDR InfiniBand (networking)\nArtemis computers (which we’ll call machines or nodes) run a Linux operating system, ‘CentOS’ v6.10. Computing performed on Artemis nodes is managed by a scheduler, and ours is an instance of ‘PBS Pro’."
  },
  {
    "objectID": "notebooks/01_context.html#specifically-what-is-different-in-artemis",
    "href": "notebooks/01_context.html#specifically-what-is-different-in-artemis",
    "title": "Introduction to High Performance Computing",
    "section": "Specifically what is different in Artemis",
    "text": "Specifically what is different in Artemis\nAs Artemis runs on a linux operating system, many of the folders and structure of them adhere to standard linux file systems. This includes a root folder / and a tree like structure from it that includes things like binaries /bin, temporary files /tmp, configuration files / etc etc etc…There are a few exceptions that are specific to Artmemis, notably /home, /project, and /scratch.\n\nhome\n/home is your home directory. Every user on Artemis (and any Linux system) has a home directory located at /home/. Your username may be your Unikey, or it may be the training accounts we’re using today. Every time you log in to Artemis, this is where you’ll end up: home\n\n\nproject\nThe /project branch is where researchers on a project should keep all the data and programs or scripts that are currently being used. We say ‘currently’ because project directories are allocated only 1 TB of storage space – that may sound like a lot, but this space is shared between all users of your project, and it runs out faster than you think. Data that you’re not currently working on should be deleted or moved back to its permanent storage location (which should be the Research Data Store1).\nThis also means that everyone working on your project can see all files in the project directory by default.\nProject directories are all subfolders of /project, and will have names like:\n/project/RDS-CORE-Training-RW\nwhich take the form /RDS-&lt;Faculty&gt;-&lt;Short_name&gt;-RW\n\n\nscratch\nEvery project also has a /scratch directory, at /scratch/. Short_name is the short name of the project as defined by dashr. /scratch is where you should actually perform your computations. What does this mean? If your workflow:\nHas big data inputs\nGenerates big data outputs\nGenerates lots of intermediate data files which are not needed afterward\nthen you should put and save this data in your /scratch space. The reasons are that you are not limited to 1 TB of space in /scratch as you are in /project, and there is also no reason to clutter up your shared project directories with temporary files. Once your computation is complete, simply copy the important or re-used inputs and outputs back to your /project folder, and delete your data in /scratch – that’s why it’s called scratch!\n\n\nKey Point: Inactive data in /project is wiped after 6 months, and in /scratch after 3 months!\nThe reason an automatic cleanup process occurs is that HPC’s are shared resources, and while its resources are large, if resources become consumed it start to not run properly for all users.\nData you want to keep should be moved onto the Research Data Store (RDS) - More on that later.\n\n\nArtemis specific applications\nWhile not exhaustive, the below is a high level list of applications available on artemis to submit and manage your scripts. We will use these commands in the Running Code on Artemis section.\nmodule - loads modules pre-installed on artemis to prime your scripts for running. Used in conjunction with avail (list available modules), load (loading modules), list (listing those which have been loaded) i.e. module load R for loading R, or module avail py for listing available modules whose name starts with py (i.e. python)\nqstat - investigate the queue qstat -x [JOB NUMBER]\nqsub - submit a script to the scheduler qsub script.pbs\npquota - Investigate usage and available memory on main folders.\ndt-script - This script is a ‘wrapper’ for running rsync in a PBS script. It enables you to perform data transfers as a “one-liner” without the need for a script. i.e. dt-script -P &lt;Project&gt; -f &lt;from&gt; -t &lt;to&gt;\nWhen specifying which queue you want your scripts to be via the pbs declarations (defaultQ is an option if you want Artemis to pick the queue based on your resources), its useful to have in mind resource limits.\n\n\n\n\n\n\n\n\n\nQueue\nMax Walltime\nMax Cores per Job / User / Node\nMemory (GB) per Node / Core\n\n\n\n\nsmall\n1 day\n24 / 128 / 24\n123 / &lt; 20\n\n\nnormal\n7 days\n96 / 128 / 32\n123 / &lt; 20\n\n\nlarge\n21 days\n288 / 288 / 32\n123 / &lt; 20\n\n\nhighmem\n21 days\n192 / 192 / 64\n123-6144 / &gt; 20\n\n\ngpu\n7 days\n252 / 252 / 36\n185 / –\n\n\ndtq\n10 days\n2 / 16 / 2\n16 / –\n\n\ninteractive\n4 hours\n4 / 4 / 4\n123 / –"
  }
]