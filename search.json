[
  {
    "objectID": "notebooks/01_context.html",
    "href": "notebooks/01_context.html",
    "title": "Artemis as a HPC",
    "section": "",
    "text": "Artemis is ideal for calculations that require:\n\nA long time to complete (long walltime)\nHigh RAM usage\nBig data input or outputs\nMultiple iterations of program inputs.\nAre using specific software that is designed to be run on HPC Systems (multiple cores or nodes run in parallel)\n\nArtemis is available free of charge to all University of Sydney researchers. You require a Unikey, and a valid Research Data Management Plan via DASHR with Artemis HPC access enabled.\nArtemis is also a great incentive to funding bodies to view your projects favourably – as they know you have the resources required to get the work done.\n\n\n\nArtemis is the University of Sydney’s High Performance Computer. Technically, Artemis is a computing cluster, which is a whole lot of individual computers networked together. At present, Artemis consists of:\n\n7,636 cores (CPUs)\n45 TB of RAM\n108 NVIDIA V100 GPUs\n1 PB of storage\n56 Gbps FDR InfiniBand (networking)\n\nArtemis computers (which we call machines or nodes) run a Linux operating system, CentOS v6.10. Computing performed on Artemis nodes is managed by a scheduler, and ours is an instance of PBS Pro.\n\n\n\nAs Artemis runs on a Linux operating system, many of the folders and structure of them adhere to standard Linux file systems. This includes a root folder / and a tree like structure from it that includes things like binaries /bin, temporary files /tmp, configuration files / etc etc etc…There are a few exceptions that are specific to Artmemis, notably /home, /project, and /scratch.\n\n\n/home is your home directory with 10gb of storage. Every user on Artemis (and any Linux system) has a home directory located at /home/&lt;username&gt;. Your username may be your Unikey, or it may be the training accounts we’re using today. Every time you log in to Artemis or submit a job, by default you will end up here, `/home/```\n\n\n\nThe /project branch is where researchers on a project should keep all the data and programs or scripts that are currently being used. We say ‘currently’ because project directories are allocated only 1 TB of storage space – that may sound like a lot, but this space is shared between all users of your project, and it runs out faster than you think. Data that you’re not currently working on should be deleted or moved back to its permanent storage location (which should be the Research Data Store1).\nThis also means that everyone working on your project can see all files in the project directory by default.\nProject directories are all subfolders of /project, and will have names like:\n/project/RDS-CORE-Training-RW\nwhich take the form /RDS-&lt;Faculty&gt;-&lt;Short_name&gt;-RW\n\n\n\nEvery project also has a /scratch directory, at /scratch/&lt;Short_name&gt;. Short_name is the short name of the project as defined by dashr. /scratch is where you should actually perform your computations. What does this mean? If your workflow:\n\nHas big data inputs\nGenerates big data outputs\nGenerates lots of intermediate data files which are not needed afterwards\n\nthen you should put and save this data in your /scratch space. The reasons are that you are not limited to 1 TB of space in /scratch as you are in /project, and there is also no reason to clutter up your shared project directories with temporary files. Once your computation is complete, simply copy the important or re-used inputs and outputs back to your /project folder (or better yet, your /rds folder), and delete your data in /scratch – that’s why it’s called scratch!\n\n\n\n\n\n\nData Deletion Policy\n\n\n\nKey Point: Inactive data in /project is wiped after 6 months, and in /scratch after 3 months! And Artemis is not backed up, at all.\n\n\nThe reason an automatic cleanup process occurs is that HPC’s are shared resources, and while its resources are large, if resources become consumed it start to not run properly for all users.\n\n\n\n\n\n\n/rds\n\n\n\nData you want to keep should be moved onto the Research Data Store (RDS) - More on that later.\n\n\n\n\n\nWhile not exhaustive, the below is a high level list of applications available on artemis to submit and manage your scripts. We will use these commands in the Running Code on Artemis section.\nmodule - loads modules pre-installed on artemis to prime your scripts for running. Used in conjunction with avail (list available modules), load (loading modules), list (listing those which have been loaded) i.e. module load R for loading R, or module avail py for listing available modules whose name starts with py (i.e. python)\nqstat - investigate the queue qstat -x [JOB NUMBER]\nqsub - submit a script to the scheduler qsub script.pbs\npquota - Investigate usage and available memory on main folders.\ndt-script - This script is a ‘wrapper’ for running rsync in a PBS script. It enables you to perform data transfers as a “one-liner” without the need for a script. i.e. dt-script -P &lt;Project&gt; -f &lt;from&gt; -t &lt;to&gt;\nWhen specifying which queue you want your scripts to be via the pbs declarations (defaultQ is an option if you want Artemis to pick the queue based on your resources), its useful to have in mind resource limits.\n\n\n\n\n\n\n\n\n\nQueue\nMax Walltime\nMax Cores per Job / User / Node\nMemory (GB) per Node / Core\n\n\n\n\nsmall\n1 day\n24 / 128 / 24\n123 / &lt; 20\n\n\nnormal\n7 days\n96 / 128 / 32\n123 / &lt; 20\n\n\nlarge\n21 days\n288 / 288 / 32\n123 / &lt; 20\n\n\nhighmem\n21 days\n192 / 192 / 64\n123-6144 / &gt; 20\n\n\ngpu\n7 days\n252 / 252 / 36\n185 / –\n\n\ndtq\n10 days\n2 / 16 / 2\n16 / –\n\n\ninteractive\n4 hours\n4 / 4 / 4\n123 / –"
  },
  {
    "objectID": "notebooks/01_context.html#why-use-artemis",
    "href": "notebooks/01_context.html#why-use-artemis",
    "title": "Artemis as a HPC",
    "section": "",
    "text": "Artemis is ideal for calculations that require:\n\nA long time to complete (long walltime)\nHigh RAM usage\nBig data input or outputs\nMultiple iterations of program inputs.\nAre using specific software that is designed to be run on HPC Systems (multiple cores or nodes run in parallel)\n\nArtemis is available free of charge to all University of Sydney researchers. You require a Unikey, and a valid Research Data Management Plan via DASHR with Artemis HPC access enabled.\nArtemis is also a great incentive to funding bodies to view your projects favourably – as they know you have the resources required to get the work done."
  },
  {
    "objectID": "notebooks/01_context.html#what-is-artemis",
    "href": "notebooks/01_context.html#what-is-artemis",
    "title": "Artemis as a HPC",
    "section": "",
    "text": "Artemis is the University of Sydney’s High Performance Computer. Technically, Artemis is a computing cluster, which is a whole lot of individual computers networked together. At present, Artemis consists of:\n\n7,636 cores (CPUs)\n45 TB of RAM\n108 NVIDIA V100 GPUs\n1 PB of storage\n56 Gbps FDR InfiniBand (networking)\n\nArtemis computers (which we call machines or nodes) run a Linux operating system, CentOS v6.10. Computing performed on Artemis nodes is managed by a scheduler, and ours is an instance of PBS Pro."
  },
  {
    "objectID": "notebooks/01_context.html#navigating-artemis",
    "href": "notebooks/01_context.html#navigating-artemis",
    "title": "Artemis as a HPC",
    "section": "",
    "text": "As Artemis runs on a Linux operating system, many of the folders and structure of them adhere to standard Linux file systems. This includes a root folder / and a tree like structure from it that includes things like binaries /bin, temporary files /tmp, configuration files / etc etc etc…There are a few exceptions that are specific to Artmemis, notably /home, /project, and /scratch.\n\n\n/home is your home directory with 10gb of storage. Every user on Artemis (and any Linux system) has a home directory located at /home/&lt;username&gt;. Your username may be your Unikey, or it may be the training accounts we’re using today. Every time you log in to Artemis or submit a job, by default you will end up here, `/home/```\n\n\n\nThe /project branch is where researchers on a project should keep all the data and programs or scripts that are currently being used. We say ‘currently’ because project directories are allocated only 1 TB of storage space – that may sound like a lot, but this space is shared between all users of your project, and it runs out faster than you think. Data that you’re not currently working on should be deleted or moved back to its permanent storage location (which should be the Research Data Store1).\nThis also means that everyone working on your project can see all files in the project directory by default.\nProject directories are all subfolders of /project, and will have names like:\n/project/RDS-CORE-Training-RW\nwhich take the form /RDS-&lt;Faculty&gt;-&lt;Short_name&gt;-RW\n\n\n\nEvery project also has a /scratch directory, at /scratch/&lt;Short_name&gt;. Short_name is the short name of the project as defined by dashr. /scratch is where you should actually perform your computations. What does this mean? If your workflow:\n\nHas big data inputs\nGenerates big data outputs\nGenerates lots of intermediate data files which are not needed afterwards\n\nthen you should put and save this data in your /scratch space. The reasons are that you are not limited to 1 TB of space in /scratch as you are in /project, and there is also no reason to clutter up your shared project directories with temporary files. Once your computation is complete, simply copy the important or re-used inputs and outputs back to your /project folder (or better yet, your /rds folder), and delete your data in /scratch – that’s why it’s called scratch!\n\n\n\n\n\n\nData Deletion Policy\n\n\n\nKey Point: Inactive data in /project is wiped after 6 months, and in /scratch after 3 months! And Artemis is not backed up, at all.\n\n\nThe reason an automatic cleanup process occurs is that HPC’s are shared resources, and while its resources are large, if resources become consumed it start to not run properly for all users.\n\n\n\n\n\n\n/rds\n\n\n\nData you want to keep should be moved onto the Research Data Store (RDS) - More on that later.\n\n\n\n\n\nWhile not exhaustive, the below is a high level list of applications available on artemis to submit and manage your scripts. We will use these commands in the Running Code on Artemis section.\nmodule - loads modules pre-installed on artemis to prime your scripts for running. Used in conjunction with avail (list available modules), load (loading modules), list (listing those which have been loaded) i.e. module load R for loading R, or module avail py for listing available modules whose name starts with py (i.e. python)\nqstat - investigate the queue qstat -x [JOB NUMBER]\nqsub - submit a script to the scheduler qsub script.pbs\npquota - Investigate usage and available memory on main folders.\ndt-script - This script is a ‘wrapper’ for running rsync in a PBS script. It enables you to perform data transfers as a “one-liner” without the need for a script. i.e. dt-script -P &lt;Project&gt; -f &lt;from&gt; -t &lt;to&gt;\nWhen specifying which queue you want your scripts to be via the pbs declarations (defaultQ is an option if you want Artemis to pick the queue based on your resources), its useful to have in mind resource limits.\n\n\n\n\n\n\n\n\n\nQueue\nMax Walltime\nMax Cores per Job / User / Node\nMemory (GB) per Node / Core\n\n\n\n\nsmall\n1 day\n24 / 128 / 24\n123 / &lt; 20\n\n\nnormal\n7 days\n96 / 128 / 32\n123 / &lt; 20\n\n\nlarge\n21 days\n288 / 288 / 32\n123 / &lt; 20\n\n\nhighmem\n21 days\n192 / 192 / 64\n123-6144 / &gt; 20\n\n\ngpu\n7 days\n252 / 252 / 36\n185 / –\n\n\ndtq\n10 days\n2 / 16 / 2\n16 / –\n\n\ninteractive\n4 hours\n4 / 4 / 4\n123 / –"
  },
  {
    "objectID": "notebooks/03_step2.html",
    "href": "notebooks/03_step2.html",
    "title": "Example PBS scripts for Python, R and Matlab",
    "section": "",
    "text": "Now that we know how to submit jobs to the scheduler lets reinforce this by running scripts that run different programming languages.\nInvestigate the computepi_fire.py python file. Given a number of trials to run, this code estimates the number pi by randomly assigning points within a square and comparing the number of points that fall within a circle of radius one relative to the total number of points.\nnano estimate_pi.pbs\nThings to note:\n\nThe cd $PBS_O_WORKDIR command changes the directory to the location where the pbs script was submitted. This is an example of a shell variable that has been set by the pbs environment.\nThe Multiprocessing package is used to create a pool of resources (with its owner python interpreter). This package is a solution to the notorious global interpreter lock GIL that prevents multiple threads from executing python at once.\nThe pbs script that runs computepi_fire.py requires a list of numbers reflecting a simulation number. The larger, the more accurate the approximation of pi. Now is a good chance to look at queue management and practice qstat -x &lt;jobnumber&gt;, qstat -u &lt;username&gt; and qdel &lt;jobnumber&gt; commands by increasing the simulation numbers."
  },
  {
    "objectID": "notebooks/03_step2.html#python",
    "href": "notebooks/03_step2.html#python",
    "title": "Example PBS scripts for Python, R and Matlab",
    "section": "",
    "text": "Now that we know how to submit jobs to the scheduler lets reinforce this by running scripts that run different programming languages.\nInvestigate the computepi_fire.py python file. Given a number of trials to run, this code estimates the number pi by randomly assigning points within a square and comparing the number of points that fall within a circle of radius one relative to the total number of points.\nnano estimate_pi.pbs\nThings to note:\n\nThe cd $PBS_O_WORKDIR command changes the directory to the location where the pbs script was submitted. This is an example of a shell variable that has been set by the pbs environment.\nThe Multiprocessing package is used to create a pool of resources (with its owner python interpreter). This package is a solution to the notorious global interpreter lock GIL that prevents multiple threads from executing python at once.\nThe pbs script that runs computepi_fire.py requires a list of numbers reflecting a simulation number. The larger, the more accurate the approximation of pi. Now is a good chance to look at queue management and practice qstat -x &lt;jobnumber&gt;, qstat -u &lt;username&gt; and qdel &lt;jobnumber&gt; commands by increasing the simulation numbers."
  },
  {
    "objectID": "notebooks/03_step2.html#matlab",
    "href": "notebooks/03_step2.html#matlab",
    "title": "Example PBS scripts for Python, R and Matlab",
    "section": "Matlab",
    "text": "Matlab\nIf you are familiar with Matlab on your local machine, using Matlab is a little different here on Artemis. High performance computers generally work best when engaging software via Linux commands. Applications that rely on graphical interfaces generally are slow and buggy (Nomachine is recommended if there is no alternative).\nqsub airline.pbs\nThings to note:\n\nThe extra flags `-nodisplay -nosplash suppress the matlab graphical interaction that become problematic on Artemis.\nThe command line way to run Matlab with the -r (run) flag. Notice the exit that closes the automatic graphical communication that Matlab automatically engages."
  },
  {
    "objectID": "notebooks/03_step2.html#r",
    "href": "notebooks/03_step2.html#r",
    "title": "Example PBS scripts for Python, R and Matlab",
    "section": "R",
    "text": "R\nRunning R code on Artemis requires installing and referencing libraries needed for your R script.\nOne way to do this is by engaging the interactive queue and via the R console on the Artemis terminal use install.package commands that specify the libary path (lib) to be a location you have writeable permissions with. As an example see the interactive demo.\nAnother approach is to use the renv package to replicate libraries and specific versions created locally via a “lockfile”. The lockfile is created by renv. Alternatives installing packages separately (most likely using a script or interactive queue) and updating the libPaths within R scripts is the simpler option if renv is not used.\nThree example scripts to demonstrate using R (with an established renv lock file) are:\n\ninstall_env.pbs: Houses the initial setup of installing R libraries to a path you have read / write permissions to. Here /home is used, but in practice it should reside somewhere in your /project directory. The installation has already been run and libraries saved in the /project/Training/DATA/Libs folder\ncreate_env.R: Demonstrates how to load an existing lock file and install packages. This is triggered by install_env.pbs\nrun_network.pbs : Main pbs script that references a path where libraries exist\nnetwork.R : The main R script that does data manipulation and visualisation via common R libraries including dplyr, plotly and network libraries. Generates a network plot pdf based on flight information\n\nqsub run_network.pbs\nThings to note:\n\nIf you have installed packages in locations that R is not expecting, it needs to know where to look for installed packages. Updating this via libPaths does this and tells R where to look for libraries.\n\n.libPaths( c( .libPaths(), \"/project/Training/DATA/Libs\") )\n\nWith the pbs script, updating paths that R expect libraries to be in can be done via export command. Example export R_LIBS_USER=$HOME/R/library\n\nMore info at the Artemis Documentation."
  },
  {
    "objectID": "notebooks/03_step1.html",
    "href": "notebooks/03_step1.html",
    "title": "Submitting a Job",
    "section": "",
    "text": "Submitting a Job\nAssuming you are logged on to Artemis with the training credentials lets run compute jobs on Artemis HPC. Please use your own unikey credentials after todays training.\nAs mentioned, pbs scripts communicate your requirements to the scheduler. These scripts tend to follow the basic pattern:\n\na, Write pbs declarations to communicate with the scheduler.\nb, Load modules required for your scripts (including environments)\nc, Run your scripts and manage directory paths and/or shell variables\n\nThe below series of commands will create a folder in a specifc location with the /project folder, extract data and rename a file which we will use to run our scripts. As we have logged in with the training credentials, we should have access to the /project/Training folder and the compressed file housed within it.\nOn the Artemis Terminal type the following commands:\ncd /project/Training\nmkdir &lt;yourname&gt;\ncd &lt;yourname&gt;\ntar -xzvf /project/Training/DATA/intro_hpc.tar.gz\ncd datahpc\ncp index.pbs basic.pbs\n\n\n\n\n\n\nNot using these notes with the Training project?\n\n\n\nUse wget https://github.com/Sydney-Informatics-Hub/IntroHPCData/archive/refs/tags/20240206.zip in place of the tar command to download all the scripts and data to use with these notes.\n\n\nCreating and running pbs files are how you communicate the the HPC Scheduler which managed your script and the resources of the HPC. Refer to the HPC Context section for more info on what the scheduler does and implications in its use.\nOpen up the nano text editor and adjust the basic.pbs with:\nnano basic.pbs\nWe will alter its contents to run a simple hello world script. Specify the Training project and defaultQ queue in the PBS declarations. Delete the existing content below the PBS declarations of basic and add the below content to create a hello world to test Artemis\nbasic.pbs should contain*:\n#!/bin/bash\n#PBS -P Training\n#PBS -N test-hello\n#PBS -l select=1:ncpus=1:mem=4gb\n#PBS -l walltime=00:10:00\n#PBS -q defaultQ\n\ncd /project/Training/datahpc/&lt;yourname&gt;\nmkdir New_job\ncp basic.pbs New_job/copy.pbs\nperl hello.pl \"YourName\"\nSubmit the basic.pbs script to the scheduler with\nqsub basic.pbs\nHow do we know if our script ran successfully? Investigate the output and error files that were automatically generated whose name refers to the #PBS -N flag and includes the job number printed when qsub was engaged."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup",
    "section": "",
    "text": "To connect to Artemis HPC, and follow this lesson, you will need a ‘terminal emulator with SSH (secure shell). SSH provides encrypted communication between the your computer and a remote server, in this case Artemis. The SSH client will be invoked by the command ‘ssh’.\nIf you are a Windows users please have one of either apps below installed:\n\nPutty (recommended)\nMobaXterm\nWSL and Ubuntu (advanced)\n\nIf you are a Mac user we will be using the Terminal which comes pre-installed on your device.\n\n\n\nFor all users, if you are not on campus install and/or have your Cisco Anyconnect VPN up and running."
  },
  {
    "objectID": "setup.html#essential-software",
    "href": "setup.html#essential-software",
    "title": "Setup",
    "section": "",
    "text": "To connect to Artemis HPC, and follow this lesson, you will need a ‘terminal emulator with SSH (secure shell). SSH provides encrypted communication between the your computer and a remote server, in this case Artemis. The SSH client will be invoked by the command ‘ssh’.\nIf you are a Windows users please have one of either apps below installed:\n\nPutty (recommended)\nMobaXterm\nWSL and Ubuntu (advanced)\n\nIf you are a Mac user we will be using the Terminal which comes pre-installed on your device.\n\n\n\nFor all users, if you are not on campus install and/or have your Cisco Anyconnect VPN up and running."
  },
  {
    "objectID": "setup.html#credentials-to-access-artemis",
    "href": "setup.html#credentials-to-access-artemis",
    "title": "Setup",
    "section": "Credentials to Access Artemis",
    "text": "Credentials to Access Artemis\nYou would normally use your unikey and unikey password to access Artemis. For the course you will be using training credentials. which will give access to the Training project:\nUsername: ict_hpctrain&lt;N&gt;, with N from 1-20 (replace  with your assigned number)\nPassword: to be provided.\n\nMac Logon to Artemis\nUse the Terminal App. Found in your Applications folder, or hit Command-Space and type terminal. Then in the terminal enter:\nssh  &lt;unikey&gt;@hpc.sydney.edu.au\n\n\nWindows Logon to Artemis\n\nOpen Putty.\nFill in the connection details:\nHost Name: hpc.sydney.edu.au\nPort: 22\nConnection type: SSH\nName this session “Artemis” and click ‘Save’"
  },
  {
    "objectID": "setup.html#working-on-artemis-with-graphical-applications",
    "href": "setup.html#working-on-artemis-with-graphical-applications",
    "title": "Setup",
    "section": "Working on Artemis with Graphical Applications",
    "text": "Working on Artemis with Graphical Applications\n\nX11 FORWARDING\nX11 forwarding allows you to run graphical applications on a remote server and have their Graphical User Interfaces (GUIs) displayed on your local machine. This is particularly useful when you are working on a remote server or computer and need to run graphical applications, such as graphical text editors, web browsers, or any other GUI-based tools.\nTo use X11 forwarding you need to install additional tools and tweak the way you connect. Recommendtions are as follows.\n\nMac Users\nYou will need to install and start Xquartz. Then log in to Artemis by providing the extra -X flag. That is:\nssh -X &lt;unikey&gt;@hpc.sydney.edu.au\n\n\nWindows Users\nYou will need to install and start Xming. Then in PuTTY, you can enable X11 forwarding in a new or saved SSH sessions by selecting Enable X11 forwarding in the PuTTY Configuration window (Connection &gt; SSH &gt; X11).\nAlternatively, MobaXterm offers a rich experience as a full-featured X-server and terminal emulator for SSH connections, the free version is more than adequate.\n\n\n\nNOMACHINE\nAn alternative to X11 forwarding is to use our graphical login nodes via the NoMachine App. These special are graphics-enabled login servers which host NoMachine, a kind of remote desktop service.\nOur experience is that no-machine is preferrable when using visualisation software that rely on computation done on the HPC.\nTo use the graphical login nodes:\nDownload and install the NoMachine Enterprise Client for your operating system. Please don’t download the client from NoMachine directly, as their current version may not work with Artemis.\nIf you’re using MacOS, you need to install XQuartz.\nDownload the glogin.nxs session configuration file to your computer.\nTo connect:\nDouble-click or run the glogin.nxs shortcut you downloaded (it will open NoMachine Client). A NoMachine window should start, asking for a username and password. Replace YOUR_UNIKEY with your UniKey, then enter your UniKey password in the password field, then click OK. A short while later, an Artemis terminal window should open."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to High Performance Computing",
    "section": "",
    "text": "Introduces Sydney’s High Performance Computer (HPC), ‘Artemis’. We cover connecting and navigating Artemis, available software, and how to submit and monitor computational jobs using the PBS Pro scheduler.\nTarget audience: Students and staff who would like to learn strategies for accelerating workloads that can be applied to common tools like matlab, python, R and more! Participants must have a valid Sydney University Unikey.\nAll data used in the course can be found here. “Download ZIP” under the green code button can be used to download it if you are unfamiliar with git."
  },
  {
    "objectID": "index.html#course-pre-requisites-and-setup-requirements",
    "href": "index.html#course-pre-requisites-and-setup-requirements",
    "title": "Introduction to High Performance Computing",
    "section": "Course pre-requisites and setup requirements",
    "text": "Course pre-requisites and setup requirements\nNo previous programming experience is required, this training course will introduce you to fundamentals of the various tools (unix/linux, HPC) as required. Training will be delivered online, so you will need access to a modern computer with a stable internet connection. Participants must have a valid Sydney University Unikey and will require the Sydney VPN and a terminal client (as per the Setup Instructions provided)."
  },
  {
    "objectID": "index.html#the-sydney-informatics-hub",
    "href": "index.html#the-sydney-informatics-hub",
    "title": "Introduction to High Performance Computing",
    "section": "The Sydney Informatics hub",
    "text": "The Sydney Informatics hub\nThis course is brought to you by Sydney Informatics Hub (SIH) is a Core Research Facility of the University of Sydney. Core Research Facilities centralise essential research equipment and services that would otherwise be too expensive or impractical for individual Faculties to purchase and maintain"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Introduction to High Performance Computing",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nIf you do use Artemis for your research, please acknowledge us! This ensures that we continue to get the funding we need to provide you with a first-grade computing resource. A suggested acknowledgment might say:\nThe authors acknowledge the Sydney Informatics Hub and the University of Sydney’s high performance computing cluster, Artemis, for providing the computing resources that have contributed to the results reported herein."
  },
  {
    "objectID": "index.html#before-training-10-mins",
    "href": "index.html#before-training-10-mins",
    "title": "Introduction to High Performance Computing",
    "section": "Before Training (10 mins)",
    "text": "Before Training (10 mins)\nTo hit the ground running during the training please:\n\nRead and install the neccessary apps in the Setup Instructions.\nRead the general HPC content under Concepts."
  },
  {
    "objectID": "notebooks/03_step3.html",
    "href": "notebooks/03_step3.html",
    "title": "Introduction to HPC",
    "section": "",
    "text": "Storing data on Artemis is not recommended due to the automatic cleaning of unused data, and the fact that data is not backed up. Its hardware is geared towards processing (compute) rather than long term storage.\nFor data storage, we have a dedicated set of machines referred to as the Research Data Store (RDS). The RDS is a central location to securely store research data for a long duration.\nAccess to RDS is granted and managed through a “Research Data Managment Plan” project completed in the Research Dashboard DASHR. More information on RDS can be found here.\nGenerally there are three ways you can transfer files to RDS.\n\nUse commands (scp or rsync) or a graphical based app (FileZilla) to move files to its remote location.\nTransfer files from Artmeis to RDS, as the RDS appears as an accessible drive on it.\nMount your RDS to your local computer.\n\nLets practice data transfer by moving the download.pbs file. “Download ZIP” under the green code button can be used to download it if you are unfamiliar with git."
  },
  {
    "objectID": "notebooks/03_step3.html#research-data-storage",
    "href": "notebooks/03_step3.html#research-data-storage",
    "title": "Introduction to HPC",
    "section": "",
    "text": "Storing data on Artemis is not recommended due to the automatic cleaning of unused data, and the fact that data is not backed up. Its hardware is geared towards processing (compute) rather than long term storage.\nFor data storage, we have a dedicated set of machines referred to as the Research Data Store (RDS). The RDS is a central location to securely store research data for a long duration.\nAccess to RDS is granted and managed through a “Research Data Managment Plan” project completed in the Research Dashboard DASHR. More information on RDS can be found here.\nGenerally there are three ways you can transfer files to RDS.\n\nUse commands (scp or rsync) or a graphical based app (FileZilla) to move files to its remote location.\nTransfer files from Artmeis to RDS, as the RDS appears as an accessible drive on it.\nMount your RDS to your local computer.\n\nLets practice data transfer by moving the download.pbs file. “Download ZIP” under the green code button can be used to download it if you are unfamiliar with git."
  },
  {
    "objectID": "notebooks/03_step3.html#graphical-transfer",
    "href": "notebooks/03_step3.html#graphical-transfer",
    "title": "Introduction to HPC",
    "section": "Graphical Transfer",
    "text": "Graphical Transfer\n\nTransfer to Artemis\nDownload FileZilla Client which is a cross platform app to transfer data to remote sites.\n\nIn the Host field enter sftp://hpc.sydney.edu.au. This is Artemis and files can be transfered from your local computer to here via a secure file transfer protocol.\nFor your Username field enter the same username that you used previously for artemis associated with training. i.e. ict_hpctrainN where N is your number between 1 and 20. In the Password field, enter the training password.\nThe Port field should by default be 22 but sometimes its better to specify it.\n\nOnce connected, you should see the home directory of Artemis on the right pane. Let navigate to the same directory we used earlier. To change to a different directory, in the Remote Site field type /project/Training/. The right pane should then reflect its contents. Navigate to the same subdirectory you used earlier that sits under Training, i.e. /project/Training/&lt;yourDirectoryName&gt;.\nOnce the appropriate locations are found, to remotely copy a file drag and drop contents from the left pane to the right.\n\n\nTransferring a file to RDS\nUsing Filezilla, transferring a file to RDS follows a similar process. You first need to connect to the RDS by:\n\nIn the Host field enter either research-data-ext.sydney.edu.au or research-data-int.sydney.edu.au. Choose the external location (first choice) if you are not on campus (and have your VPN up and running), otherwise the internal (second) choice if you are at campus.\nCredentials and Port number as before.\n\nOnce connected, in the Remote Site field type /rds/PRJ-Training and enter. Contents on the right pane should now be visible. Once you begin to use your own credentials for your own projects, use /rds/PRJ-NameOfProject instead, where the NameOfProject is the short name of your project as dashr knows it to be."
  },
  {
    "objectID": "notebooks/03_step3.html#using-commands",
    "href": "notebooks/03_step3.html#using-commands",
    "title": "Introduction to HPC",
    "section": "Using commands",
    "text": "Using commands\nEither scp or rsync commands in your Terminal (Mac and linux: Terminal, Windows: Windows Terminal or Powershell equivalent) can be used as an alternative to graphical applications. Secure copy (scp) is demonstrated below. You will be asked to provide a password (your key strokes will not be shown) and once verified, a status bar will then be displayed showing your copy progress.\nOn your Local Terminal type:\nscp dogScripts.tar.gz  ict_hpctrain&lt;N&gt;@hpc.sydney.edu.au:/project/Training/&lt;yourDirectoryName&gt;\nWhere  where N is your number between 1 and 20."
  },
  {
    "objectID": "notebooks/03_step3.html#moving-data-while-on-artemis",
    "href": "notebooks/03_step3.html#moving-data-while-on-artemis",
    "title": "Introduction to HPC",
    "section": "Moving data while on Artemis",
    "text": "Moving data while on Artemis\n\nUsing the data transfer queue\nCopying small files the above ways is great - except when the files are considerable large. Interuptions to your interconnect connection can corrupt a large transfer. If you are on Artemis, we can instead ‘wrap’ file transfers in a PBS script to ensure your transfer are not hampered by interuptions.\nLets practice. Using commands on the Artemis Terminal extract the compressed file by typing tar -zxvf dogScripts.tar.gz. Once files are extracted submit the file transfer to the scheudler by typing qsub download.pbs. Notice the in the download.pbs file queue specified is the datatransfer queue.\n\n\nTransfer between RDS and Artemis\nAs mentioned, the RDS appears as a mount on Artemis. Hence the usual linux commands like cp or rsync are the standard way to transfer files between Artemis and RDS. The following command on the Artemis Terminal demonstrates file transfer:\ncp -v /rds/PRJ-Training/Dog_disease/Ref/* /project/Training/&lt;yourname&gt;\nThere is an even easier way! Our HPC’s management team have created a handy utility for us called dt-script. This script (full path /usr/local/bin/dt-script for the interested) is a ‘wrapper’ for running rsync in a PBS script. It enables you to perform data transfers as a “one-liner” without the need for a script and the transfer is actually submitted to the data transfer nodes of the cluster (and not running in the foreground of your current terminal).\nThe syntax for using dt-script is dt-script -P  -f  -t \nwhere -P is the dashr project (short name), from (-f) directory and to (-t) a directory.\nThe following steps may work best on the Artemis Production Instance\nmodule unload training\nOn the Artemis Terminal:\ndt-script -P Training -f /rds/PRJ-Training/Dog_disease/Ref/ -t /project/Training/&lt;yourDirectoryName&gt;/\nThe reverse is the easiest way possible to back up your project on Artemis. Assuming you are situated within the directory containing the data to backup (so that you can make use of pwd shortcut). On the Artemis Terminal:\ndt-script -P Training -N backup&lt;yourName&gt; -f `pwd` -t /rds/PRJ-Training"
  },
  {
    "objectID": "notebooks/03_step3.html#mounting-your-rds-on-your-drive",
    "href": "notebooks/03_step3.html#mounting-your-rds-on-your-drive",
    "title": "Introduction to HPC",
    "section": "Mounting your RDS on your drive",
    "text": "Mounting your RDS on your drive\nKeep in mind you must have your VPN on or be at campus for connection to establish.\n\nThese steps for Windows 10 are:\n\nClick on This PC from the Desktop.\nOn the Computer tab, click on Map network drive in the Network section. Choose a drive letter and enter you the path: \\\\shared.sydney.edu.au\\research-data. When asked, please enter shared&lt;unikey&gt; for the user name.\nClick Finish.\n\n\n\nThese steps for Mac are:\nTo mount on Linux or Mac operating systems, you can use the smb network communication protocol (also known as CIFS) by mounting the path.\n\nFinder &gt; Go &gt; Connect to Server.\nIn the Server Address use: smb://shared.sydney.edu.au/research-data\nEnter your credentials as a Registered User.\n\n\n\nThese steps for Linux are:\nThe command line is the easiest way to mount the RDS is using a tool called sshfs which connects via the ssh protocol and performs file transfers using sftp.\n\nFirstly install sshfs sudo apt install sshfs.\nCreate a directory where you want the mounted data to reside mkdir /home/ubuntu/myRDS and mount away sshfs &lt;unikey&gt;@research-data-int.sydney.edu.au:/rds/PRJ-&lt;yourproject&gt; /home/ubuntu/myRDS."
  },
  {
    "objectID": "notebooks/03_step4.html",
    "href": "notebooks/03_step4.html",
    "title": "Introduction to HPC",
    "section": "",
    "text": "Research Computing Wiki\nLearning Linux via Software Carpentry Course\nHPC Options other than Artemis\nUniversity of Sydney Research Cloud\nNCI\nSydney Informatics Hub HPC Scheme"
  },
  {
    "objectID": "notebooks/03_step4.html#more-resources",
    "href": "notebooks/03_step4.html#more-resources",
    "title": "Introduction to HPC",
    "section": "",
    "text": "Research Computing Wiki\nLearning Linux via Software Carpentry Course\nHPC Options other than Artemis\nUniversity of Sydney Research Cloud\nNCI\nSydney Informatics Hub HPC Scheme"
  },
  {
    "objectID": "notebooks/02_connect.html",
    "href": "notebooks/02_connect.html",
    "title": "Artemis as a Remote Server",
    "section": "",
    "text": "Artemis runs on a Linux operating system (currently CentOS v6.10). Becoming familiar with Linux commands can be quite challenging for those users who are used to graphical systems such as Windows. A Basic knowledge of Linux is a preferable for this course. A recommended course to learn the basics is by Software Carpentry.\nUsing Linux involves interacting with the system through commands entered in a command-line interface (CLI). The pattern of commands tend to follow:\ncommand [options] [arguments]\nFor instance, This command on your local terminal ls -ltr ~ lists all files in the home directory.\n(ls) is the command. Options include: long format (-l), sorted by modification time (-t) in reverse order with the most recently modified files appearing at the bottom (-r). The argument is (~) which is shorthand for the home directory.\nnano is the text editor we will be using to make and adjust files and is preinstalled on Artemis. Using nano will be demonstrated.\n\n\n\nOn Artemis, PBS Pro is the name of the scheduler that manages computer resources and jobs (what people want to run). The scheduler manages available capacity and the competing jobs wanting to access its resources via a queue. One a job is submitted to the scheduler (via qsub command ), a job is placed in the queue until spare capacity is found to run it. Generally the larger the resources you are requesting, the longer your job will be placed in a queue.\nPBS Scripts are how we communicate our requirements to the scheduler. PBS Declarations are the syntax of doing so and is a combination of a directive, a flag and options. All lines starting with #PBS are PBS declarations and the scheduler will try to interpret its meaning. An example of a PBS declaration is:\n#PBS -l select=1:ncpus=8:mem=16gb\nThis specifies a resource request with the flag -l, consisting of 1 node (select=1), 8 CPUs (ncpus=8) and 16 GB of RAM (mem=16GB).\nA series of PBS declarations are housed within a PBS script.\nResource requests are usually a combination of:\n\nselect: the number of compute nodes (most often 1 unless MPI is being used)\nncpus: the number of CPU cores\nmem: the amount of RAM\nngpus: the number of GPU cores\nwalltime: the length of time all these resources will be made available to the requesting job\n\nTo highlight walltime, you only have the collect of resources for a defined duration. If you have used your allocated walltime, the scheduler will stop your task irrespective of if your job is still running.\nGenerally the larger the resources requests, the longer (given spare capacity of the HPC and competing jobs) your job will most likely be placed in the queue before running. The scheduler controls the queue.\nCommon PBS Flags are:\n\n\n\n\n\n\n\n\nOption\nDescription\nNotes\n\n\n\n\n-P\nProject short name\nRequired directive on Artemis\n\n\n-N\nJob name\nName it whatever you like; no spaces\n\n\n-l\nResource request\na combination of resources as mentioned\n\n\n-q\nJob queue\nDefaults to defaultQ. Use dtq for data transfer only\n\n\n-M\nEmail address\nNotifications can be sent by email on certain events\n\n\n-m\nEmail options: abe\nReceive notification on (a)bort, (b)egin, or (e)nd of jobs\n\n\n-I\nInteractive mode\nOpens a shell terminal with access to the requested resources\n\n\n\nOther HPC systems might use other Schedulers requiring a different syntax (SLURM), but the concepts are the same.\n\n\n\nOnce you logon on to Artemis you are placed on the login nodes. Avoid running scripts there as you wont be able to access the potential resources available. PBS script submitted to the scheduler ensure your job is transferred from the login nodes to the compute nodes that access larger resources."
  },
  {
    "objectID": "notebooks/02_connect.html#linux-background",
    "href": "notebooks/02_connect.html#linux-background",
    "title": "Artemis as a Remote Server",
    "section": "",
    "text": "Artemis runs on a Linux operating system (currently CentOS v6.10). Becoming familiar with Linux commands can be quite challenging for those users who are used to graphical systems such as Windows. A Basic knowledge of Linux is a preferable for this course. A recommended course to learn the basics is by Software Carpentry.\nUsing Linux involves interacting with the system through commands entered in a command-line interface (CLI). The pattern of commands tend to follow:\ncommand [options] [arguments]\nFor instance, This command on your local terminal ls -ltr ~ lists all files in the home directory.\n(ls) is the command. Options include: long format (-l), sorted by modification time (-t) in reverse order with the most recently modified files appearing at the bottom (-r). The argument is (~) which is shorthand for the home directory.\nnano is the text editor we will be using to make and adjust files and is preinstalled on Artemis. Using nano will be demonstrated."
  },
  {
    "objectID": "notebooks/02_connect.html#the-scheduler.",
    "href": "notebooks/02_connect.html#the-scheduler.",
    "title": "Artemis as a Remote Server",
    "section": "",
    "text": "On Artemis, PBS Pro is the name of the scheduler that manages computer resources and jobs (what people want to run). The scheduler manages available capacity and the competing jobs wanting to access its resources via a queue. One a job is submitted to the scheduler (via qsub command ), a job is placed in the queue until spare capacity is found to run it. Generally the larger the resources you are requesting, the longer your job will be placed in a queue.\nPBS Scripts are how we communicate our requirements to the scheduler. PBS Declarations are the syntax of doing so and is a combination of a directive, a flag and options. All lines starting with #PBS are PBS declarations and the scheduler will try to interpret its meaning. An example of a PBS declaration is:\n#PBS -l select=1:ncpus=8:mem=16gb\nThis specifies a resource request with the flag -l, consisting of 1 node (select=1), 8 CPUs (ncpus=8) and 16 GB of RAM (mem=16GB).\nA series of PBS declarations are housed within a PBS script.\nResource requests are usually a combination of:\n\nselect: the number of compute nodes (most often 1 unless MPI is being used)\nncpus: the number of CPU cores\nmem: the amount of RAM\nngpus: the number of GPU cores\nwalltime: the length of time all these resources will be made available to the requesting job\n\nTo highlight walltime, you only have the collect of resources for a defined duration. If you have used your allocated walltime, the scheduler will stop your task irrespective of if your job is still running.\nGenerally the larger the resources requests, the longer (given spare capacity of the HPC and competing jobs) your job will most likely be placed in the queue before running. The scheduler controls the queue.\nCommon PBS Flags are:\n\n\n\n\n\n\n\n\nOption\nDescription\nNotes\n\n\n\n\n-P\nProject short name\nRequired directive on Artemis\n\n\n-N\nJob name\nName it whatever you like; no spaces\n\n\n-l\nResource request\na combination of resources as mentioned\n\n\n-q\nJob queue\nDefaults to defaultQ. Use dtq for data transfer only\n\n\n-M\nEmail address\nNotifications can be sent by email on certain events\n\n\n-m\nEmail options: abe\nReceive notification on (a)bort, (b)egin, or (e)nd of jobs\n\n\n-I\nInteractive mode\nOpens a shell terminal with access to the requested resources\n\n\n\nOther HPC systems might use other Schedulers requiring a different syntax (SLURM), but the concepts are the same."
  },
  {
    "objectID": "notebooks/02_connect.html#login-vs-compute-nodes",
    "href": "notebooks/02_connect.html#login-vs-compute-nodes",
    "title": "Artemis as a Remote Server",
    "section": "",
    "text": "Once you logon on to Artemis you are placed on the login nodes. Avoid running scripts there as you wont be able to access the potential resources available. PBS script submitted to the scheduler ensure your job is transferred from the login nodes to the compute nodes that access larger resources."
  }
]